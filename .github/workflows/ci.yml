name: Unified Platform CI/CD

# This workflow includes auto-formatting capabilities:
# - If Rust code formatting fails, it will automatically run `cargo fmt --all`
# - The formatted code will be committed and pushed back to the branch
# - The CI will then re-run to verify the formatting is correct
# - This only happens on push events, not on pull requests

on:
  push:
    branches: [ main ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  IMAGE_BASE: ${{ github.repository }}

jobs:
  # Change detection job
  changes:
    runs-on: ubuntu-latest
    outputs:
      claude-code: ${{ steps.filter.outputs.claude-code }}
      orchestrator: ${{ steps.filter.outputs.orchestrator }}
      taskrun: ${{ steps.filter.outputs.taskrun }}
      infra: ${{ steps.filter.outputs.infra }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v2
        id: filter
        with:
          filters: |
            claude-code:
              - 'Dockerfile'
              - 'package*.json'
              - 'src/**'
              - 'scripts/**'
            orchestrator:
              - 'orchestrator/**'
              - '!orchestrator/**/*.md'
            taskrun:
              - 'orchestrator/orchestrator-core/src/controllers/taskrun.rs'
              - 'orchestrator/orchestrator-core/src/crds/**'
              - 'infra/crds/**'
            infra:
              - 'infra/**'
              - '.github/workflows/**'

  # Version determination
  version:
    runs-on: ubuntu-latest
    outputs:
      version: ${{ steps.version.outputs.version }}
      is-release: ${{ steps.version.outputs.is-release }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Determine version
        id: version
        run: |
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            VERSION="${{ github.ref_name }}"
            echo "version=${VERSION}" >> $GITHUB_OUTPUT
            echo "is-release=true" >> $GITHUB_OUTPUT
          else
            # Use commit SHA for non-release builds
            VERSION="main-$(git rev-parse --short HEAD)"
            echo "version=${VERSION}" >> $GITHUB_OUTPUT
            echo "is-release=false" >> $GITHUB_OUTPUT
          fi

  # Parallel linting and testing
  lint-rust:
    needs: changes
    if: needs.changes.outputs.orchestrator == 'true' || needs.changes.outputs.taskrun == 'true' || github.event_name == 'push'
    runs-on: [self-hosted, k8s-runner, rust-builder, agent-platform]
    # No container needed - the runner itself uses the rust-builder image with pre-warmed cache!
    env:
      # Optimized build environment
      CARGO_INCREMENTAL: 0
      RUSTC_WRAPPER: sccache
      SCCACHE_CACHE_SIZE: 50G
      SCCACHE_IDLE_TIMEOUT: 0
      CARGO_NET_GIT_FETCH_WITH_CLI: true
      CARGO_REGISTRIES_CRATES_IO_PROTOCOL: sparse
      # PVC-based cache locations
      CARGO_HOME: /cache/cargo
      RUSTUP_HOME: /cache/rustup
      SCCACHE_DIR: /cache/sccache
      CARGO_TARGET_DIR: /cache/target
    outputs:
      format-needed: ${{ steps.format-check.outputs.format-needed }}
    steps:
      - uses: actions/checkout@v4
      - name: Setup sccache
        run: |
          # Start sccache server (may already be running from runner setup)
          sccache --start-server || echo "sccache server already running or failed to start"

          echo "üìä Initial sccache stats:"
          sccache --show-stats

      - name: Check formatting
        id: format-check
        working-directory: ./orchestrator
        run: |
          if ! cargo fmt --all -- --check; then
            echo "format-needed=true" >> $GITHUB_OUTPUT
            echo "‚ùå Code formatting issues detected"
            echo "FORMATTING_FAILED=true" >> $GITHUB_ENV
          else
            echo "format-needed=false" >> $GITHUB_OUTPUT
            echo "‚úÖ Code formatting is correct"
            echo "FORMATTING_FAILED=false" >> $GITHUB_ENV
          fi

      - name: Fail job if formatting issues detected and not on push event
        if: env.FORMATTING_FAILED == 'true' && github.event_name != 'push'
        run: |
          echo "‚ùå Formatting issues detected and auto-format is not available (not a push event)"
          exit 1

      - name: Run Clippy
        if: env.FORMATTING_FAILED == 'false' || github.event_name == 'push'
        working-directory: ./orchestrator
        run: |
          echo "üîç Running Clippy with optimized caching..."
          cargo clippy --all-targets --all-features -- -D warnings

          echo "üìä Final sccache stats:"
          sccache --show-stats

  # Auto-format job that runs when formatting fails
  auto-format:
    needs: [changes, lint-rust]
    if: always() && needs.lint-rust.outputs.format-needed == 'true' && github.event_name == 'push'
    runs-on: [self-hosted, k8s-runner, rust-builder, agent-platform]
    # No container needed - the runner itself uses the rust-builder image with pre-warmed cache!
    env:
      # Optimized build environment
      CARGO_INCREMENTAL: 0
      RUSTC_WRAPPER: sccache
      SCCACHE_CACHE_SIZE: 50G
      SCCACHE_IDLE_TIMEOUT: 0
      CARGO_NET_GIT_FETCH_WITH_CLI: true
      CARGO_REGISTRIES_CRATES_IO_PROTOCOL: sparse
      # PVC-based cache locations
      CARGO_HOME: /cache/cargo
      RUSTUP_HOME: /cache/rustup
      SCCACHE_DIR: /cache/sccache
      CARGO_TARGET_DIR: /cache/target
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4
        with:
          # Use a personal access token to trigger subsequent workflows
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Setup sccache
        run: |
          # Start sccache server (may already be running from runner setup)
          sccache --start-server || echo "sccache server already running or failed to start"

      - name: Auto-format Rust code
        working-directory: ./orchestrator
        run: |
          echo "üîß Auto-formatting Rust code..."
          cargo fmt --all

      - name: Check for changes
        id: changes
        run: |
          if git diff --quiet; then
            echo "No changes after formatting"
            echo "has-changes=false" >> $GITHUB_OUTPUT
          else
            echo "Changes detected after formatting"
            echo "has-changes=true" >> $GITHUB_OUTPUT
          fi

      - name: Commit and push changes
        if: steps.changes.outputs.has-changes == 'true'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add -A
          git commit -m "üîß Auto-format Rust code

          This commit was automatically generated by the CI/CD pipeline
          to fix formatting issues detected by cargo fmt.

          Changes made:
          - Applied cargo fmt --all to fix formatting

          [skip ci]"
          git push

      - name: Comment on commit
        if: steps.changes.outputs.has-changes == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const { owner, repo } = context.repo;
            const sha = context.sha;

            await github.rest.repos.createCommitComment({
              owner,
              repo,
              commit_sha: sha,
              body: `üîß **Auto-formatting applied**

              The CI/CD pipeline detected formatting issues and automatically applied \`cargo fmt\` to fix them.

              A new commit has been pushed with the formatting fixes. The CI will re-run automatically.

              üìù **Next time**: Run \`cargo fmt --all\` locally before pushing to avoid this automatic formatting.`
            });

  # Re-run lint after auto-format (only if auto-format ran)
  lint-rust-retry:
    needs: [changes, auto-format]
    if: always() && needs.auto-format.result == 'success'
    runs-on: [self-hosted, k8s-runner, rust-builder, agent-platform]
    # No container needed - the runner itself uses the rust-builder image with pre-warmed cache!
    env:
      # Optimized build environment
      CARGO_INCREMENTAL: 0
      RUSTC_WRAPPER: sccache
      SCCACHE_CACHE_SIZE: 50G
      SCCACHE_IDLE_TIMEOUT: 0
      CARGO_NET_GIT_FETCH_WITH_CLI: true
      CARGO_REGISTRIES_CRATES_IO_PROTOCOL: sparse
      # PVC-based cache locations
      CARGO_HOME: /cache/cargo
      RUSTUP_HOME: /cache/rustup
      SCCACHE_DIR: /cache/sccache
      CARGO_TARGET_DIR: /cache/target
    steps:
      - uses: actions/checkout@v4
        with:
          # Fetch the latest commit (including auto-format changes)
          ref: ${{ github.ref }}

      - name: Setup sccache
        run: |
          # Start sccache server (may already be running from runner setup)
          sccache --start-server || echo "sccache server already running or failed to start"

          echo "üìä Initial sccache stats:"
          sccache --show-stats

      - name: Verify formatting is now correct
        working-directory: ./orchestrator
        run: |
          echo "üîç Verifying formatting after auto-format..."
          cargo fmt --all -- --check

      - name: Run Clippy
        working-directory: ./orchestrator
        run: |
          echo "üîç Running Clippy after auto-format..."
          cargo clippy --all-targets --all-features -- -D warnings

          echo "üìä Final sccache stats:"
          sccache --show-stats

  test-rust:
    needs: changes
    if: needs.changes.outputs.orchestrator == 'true' || needs.changes.outputs.taskrun == 'true' || github.event_name == 'push'
    runs-on: [self-hosted, k8s-runner, rust-builder, agent-platform]
    # No container needed - the runner itself uses the rust-builder image with pre-warmed cache!
    env:
      # Optimized build environment
      CARGO_INCREMENTAL: 0
      RUSTC_WRAPPER: sccache
      SCCACHE_CACHE_SIZE: 50G
      SCCACHE_IDLE_TIMEOUT: 0
      CARGO_NET_GIT_FETCH_WITH_CLI: true
      CARGO_REGISTRIES_CRATES_IO_PROTOCOL: sparse
      # PVC-based cache locations
      CARGO_HOME: /cache/cargo
      RUSTUP_HOME: /cache/rustup
      SCCACHE_DIR: /cache/sccache
      CARGO_TARGET_DIR: /cache/target
    steps:
      - uses: actions/checkout@v4
      - name: Setup sccache
        run: |
          # Start sccache server (may already be running from runner setup)
          sccache --start-server || echo "sccache server already running or failed to start"

          echo "üìä Initial sccache stats:"
          sccache --show-stats

      - name: Run tests
        working-directory: ./orchestrator
        run: |
          echo "üß™ Running tests with optimized caching..."
          cargo test --all-features --all-targets

          echo "üìä Final sccache stats:"
          sccache --show-stats

  # Integration tests
  integration-tests:
    needs: [lint-rust, test-rust, lint-rust-retry]
    if: always() && !cancelled() && (needs.changes.outputs.orchestrator == 'true' || needs.changes.outputs.taskrun == 'true' || needs.changes.outputs.infra == 'true' || github.event_name == 'push') && (needs.lint-rust.result == 'success' || needs.lint-rust-retry.result == 'success')
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4

      # Cache kubectl binary
      - name: Cache kubectl
        uses: actions/cache@v4
        with:
          path: /usr/local/bin/kubectl
          key: kubectl-agent-platform-${{ runner.os }}-1.30.0

      - name: Create Kind cluster
        uses: helm/kind-action@v1.8.0
        with:
          cluster_name: platform-test
          config: .github/kind-config.yaml
          kubectl_version: v1.30.0
          # Wait longer for cluster to be ready
          wait: 120s
          verbosity: 1

      - name: Install CRDs
        run: |
          echo "Installing TaskRun CRD..."
          kubectl apply -f infra/crds/taskrun-crd.yaml
          echo "Waiting for CRD to be established..."
          kubectl wait --for condition=established --timeout=60s crd/taskruns.orchestrator.io
          echo "CRD installed successfully"

      - name: Test Helm Chart Structure
        run: |
          echo "üéØ Testing orchestrator Helm chart..."

          # Install Helm
          curl -fsSL https://get.helm.sh/helm-v3.14.0-linux-amd64.tar.gz | tar xz
          sudo mv linux-amd64/helm /usr/local/bin/
          helm version

          # Lint the Helm chart
          helm lint ./infra/charts/orchestrator

          # Test template generation with different values
          echo "üìã Testing default values..."
          helm template orchestrator ./infra/charts/orchestrator > /tmp/default.yaml
          kubectl --dry-run=client apply -f /tmp/default.yaml

          echo "üìã Testing with secrets..."
          helm template orchestrator ./infra/charts/orchestrator \
            --set secrets.anthropicApiKey="test-key" \
            --set secrets.githubToken="test-token" > /tmp/with-secrets.yaml
          kubectl --dry-run=client apply -f /tmp/with-secrets.yaml

          echo "üìã Testing with custom image..."
          helm template orchestrator ./infra/charts/orchestrator \
            --set image.repository="custom/repo" \
            --set image.tag="v1.2.3" > /tmp/custom-image.yaml
          kubectl --dry-run=client apply -f /tmp/custom-image.yaml

          echo "‚úÖ Helm chart validation passed!"

      - name: Deploy test resources
        run: |
          # Create test namespace
          kubectl create namespace test-platform

          # Apply test TaskRun
          kubectl apply -f infra/crds/test-taskrun.yaml -n test-platform || true

      - name: Verify CRD deployment
        run: |
          kubectl get crd taskruns.orchestrator.io
          kubectl get taskruns -A

  # Security scanning (non-blocking, self-hosted)
  security-scan:
    needs: changes
    if: github.event_name == 'push'  # Only on main branch
    runs-on: ubuntu-latest
    continue-on-error: true  # Non-blocking
    steps:
      - uses: actions/checkout@v4

      # Cache Trivy DB
      - name: Cache Trivy database
        uses: actions/cache@v4
        with:
          path: ~/.cache/trivy
          key: trivy-db-${{ runner.os }}-${{ github.run_id }}
          restore-keys: |
            trivy-db-${{ runner.os }}-

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          cache-dir: ~/.cache/trivy

      - name: Upload Trivy scan results
        uses: actions/upload-artifact@v4
        with:
          name: trivy-results
          path: trivy-results.sarif

      # Cache cargo-audit binary
      - name: Cache cargo-audit
        uses: actions/cache@v4
        id: cargo-audit-cache
        with:
          path: ~/.cargo/bin/cargo-audit
          key: cargo-audit-${{ runner.os }}-0.21.2

      - name: Install cargo-audit
        if: steps.cargo-audit-cache.outputs.cache-hit != 'true'
        run: cargo install cargo-audit --version 0.21.2

      - name: Rust security audit
        working-directory: ./orchestrator
        run: cargo audit || true  # Non-blocking

  # Test coverage reporting (non-blocking)
  test-coverage:
    needs: changes
    if: needs.changes.outputs.orchestrator == 'true' || needs.changes.outputs.taskrun == 'true' || github.event_name == 'push'
    runs-on: [self-hosted, k8s-runner, rust-builder, agent-platform]
    # No container needed - the runner itself uses the rust-builder image with pre-warmed cache!
    env:
      # Optimized build environment
      CARGO_INCREMENTAL: 0
      RUSTC_WRAPPER: sccache
      SCCACHE_CACHE_SIZE: 50G
      SCCACHE_IDLE_TIMEOUT: 0
      CARGO_NET_GIT_FETCH_WITH_CLI: true
      CARGO_REGISTRIES_CRATES_IO_PROTOCOL: sparse
      # PVC-based cache locations
      CARGO_HOME: /cache/cargo
      RUSTUP_HOME: /cache/rustup
      SCCACHE_DIR: /cache/sccache
      CARGO_TARGET_DIR: /cache/target
    continue-on-error: true  # Non-blocking
    steps:
      - uses: actions/checkout@v4

      # Install cargo-llvm-cov for coverage
      - name: Cache cargo-llvm-cov
        uses: actions/cache@v4
        id: cargo-llvm-cov-cache
        with:
          path: ~/.cargo/bin/cargo-llvm-cov
          key: cargo-llvm-cov-${{ runner.os }}-0.6.8

      - name: Install cargo-llvm-cov
        if: steps.cargo-llvm-cov-cache.outputs.cache-hit != 'true'
        run: cargo install cargo-llvm-cov --version 0.6.8

      - name: Setup sccache
        run: |
          # Start sccache server (may already be running from runner setup)
          sccache --start-server || echo "sccache server already running or failed to start"

          echo "üìä Initial sccache stats:"
          sccache --show-stats

      - name: Generate test coverage
        working-directory: ./orchestrator
        run: |
          echo "üìä Generating test coverage report..."
          cargo llvm-cov --all-features --workspace --lcov --output-path lcov.info || true
          cargo llvm-cov --all-features --workspace --html --output-dir coverage-html || true

          echo "üìä Final sccache stats:"
          sccache --show-stats

          # Generate summary
          echo "## Test Coverage Summary" > coverage-summary.md
          echo "" >> coverage-summary.md
          if [ -f "lcov.info" ]; then
            # Extract coverage percentage from lcov file
            COVERAGE=$(grep -E "^LF:|^LH:" lcov.info | awk -F: '{if($1=="LF") lf+=$2; if($1=="LH") lh+=$2} END {if(lf>0) printf "%.1f", (lh/lf)*100; else print "0.0"}')
            echo "üìä **Overall Coverage**: ${COVERAGE}%" >> coverage-summary.md
            echo "Coverage: ${COVERAGE}%"
          else
            echo "‚ö†Ô∏è **Coverage**: Could not generate coverage report" >> coverage-summary.md
            echo "Coverage report generation failed"
          fi

          echo "" >> coverage-summary.md
          echo "üìÅ Detailed HTML report available in artifacts" >> coverage-summary.md

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            orchestrator/lcov.info
            orchestrator/coverage-html/
            orchestrator/coverage-summary.md

      - name: Display coverage summary
        working-directory: ./orchestrator
        run: |
          if [ -f "coverage-summary.md" ]; then
            echo "üìä Test Coverage Results:"
            cat coverage-summary.md
          else
            echo "‚ö†Ô∏è Coverage summary not available"
          fi

  # Build images
  build-claude-code:
    needs: [version, lint-rust, test-rust, integration-tests, lint-rust-retry]
    if: always() && !cancelled() && (needs.changes.outputs.claude-code == 'true' || github.event_name == 'push')
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4

      - name: Get Claude Code CLI version
        id: claude-version
        run: |
          CLAUDE_VERSION=$(npm view @anthropic-ai/claude-code version)
          echo "CLAUDE_VERSION=$CLAUDE_VERSION" >> $GITHUB_OUTPUT
          echo "üì¶ Latest Claude Code CLI version on npm: $CLAUDE_VERSION"

      - name: Log in to Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Check if Claude Code image exists
        id: claude-exists
        run: |
          echo "üîç Checking if we have built Claude Code version ${{ steps.claude-version.outputs.CLAUDE_VERSION }}"
          echo "Looking for: ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ steps.claude-version.outputs.CLAUDE_VERSION }}"
          if docker manifest inspect ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ steps.claude-version.outputs.CLAUDE_VERSION }} > /dev/null 2>&1; then
            echo "‚úÖ We already have this version built, skipping build"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "üèóÔ∏è We haven't built this version yet, will build"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Set up Docker Buildx
        if: steps.claude-exists.outputs.exists == 'false'
        uses: docker/setup-buildx-action@v3

      - name: Build and push Claude Code image
        if: steps.claude-exists.outputs.exists == 'false'
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          platforms: linux/amd64,linux/arm64
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:latest
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ needs.version.outputs.version }}
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ steps.claude-version.outputs.CLAUDE_VERSION }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILDKIT_INLINE_CACHE=1
            CLAUDE_CODE_VERSION=${{ steps.claude-version.outputs.CLAUDE_VERSION }}

      - name: Tag existing image with platform version
        if: steps.claude-exists.outputs.exists == 'true' && github.event_name != 'pull_request'
        run: |
          echo "üè∑Ô∏è Tagging existing image with platform version"
          docker buildx imagetools create \
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ steps.claude-version.outputs.CLAUDE_VERSION }} \
            --tag ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ needs.version.outputs.version }}

  build-orchestrator:
    needs: [version, lint-rust, test-rust, integration-tests, lint-rust-retry]
    if: always() && !cancelled() && (needs.changes.outputs.orchestrator == 'true' || github.event_name == 'push')
    runs-on: [self-hosted, k8s-runner, rust-builder, agent-platform]
    permissions:
      contents: read
      packages: write
    env:
      # Optimized build environment
      CARGO_INCREMENTAL: 0
      RUSTC_WRAPPER: sccache
      SCCACHE_CACHE_SIZE: 50G
      SCCACHE_IDLE_TIMEOUT: 0
      CARGO_NET_GIT_FETCH_WITH_CLI: true
      CARGO_REGISTRIES_CRATES_IO_PROTOCOL: sparse
    steps:
      - uses: actions/checkout@v4

      - name: Setup optimized Rust environment
        run: |
          echo "üèóÔ∏è Building orchestrator binary with pre-warmed dependencies..."
          echo "Build environment:"
          echo "Available cores: $(nproc)"
          echo "RUSTFLAGS: $RUSTFLAGS"
          echo "RUSTC_WRAPPER: $RUSTC_WRAPPER"
          echo "CARGO_HOME: $CARGO_HOME"
          echo "SCCACHE_DIR: $SCCACHE_DIR"

      - name: Setup sccache
        run: |
          # Start sccache server (may already be running from runner setup)
          sccache --start-server || echo "sccache server already running or failed to start"

          echo "üìä Initial sccache stats:"
          sccache --show-stats

      - name: Verify pre-warmed dependencies
        working-directory: ./orchestrator
        run: |
          echo "üîç Verifying pre-warmed dependencies..."
          echo "Cargo cache contents:"
          ls -la $CARGO_HOME/registry/cache/ | head -10 || echo "No cache found"
          echo ""
          echo "Target cache contents:"
          ls -la /cache/target/ | head -10 || echo "No target cache found"

      - name: Build release binary (ultra-fast)
        working-directory: ./orchestrator
        run: |
          echo "‚è±Ô∏è Starting build..."
          cargo build --release --package orchestrator-core

          # Show final sccache stats
          echo "üìä Final sccache stats:"
          sccache --show-stats

          # Copy binary from cache target dir (where CARGO_TARGET_DIR points)
          cp /cache/target/release/orchestrator-core orchestrator

          # Show binary info
          echo "üì¶ Built binary info:"
          ls -lh orchestrator
          file orchestrator

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Orchestrator image
        uses: docker/build-push-action@v5
        with:
          context: ./orchestrator
          file: ./orchestrator/Dockerfile
          platforms: linux/amd64,linux/arm64
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/orchestrator:latest
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/orchestrator:${{ needs.version.outputs.version }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          # Add registry cache for better layer reuse
          build-args: |
            BUILDKIT_INLINE_CACHE=1

  build-gemini-cli:
    needs: [version, lint-rust, test-rust, integration-tests, lint-rust-retry]
    if: always() && !cancelled() && github.event_name == 'push'  # Always build Gemini CLI on push
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout platform repository
        uses: actions/checkout@v4

      - name: Checkout Gemini CLI repository
        uses: actions/checkout@v4
        with:
          repository: google-gemini/gemini-cli
          ref: main
          path: gemini-cli

      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: gemini-cli/package-lock.json

      - name: Install dependencies and build packages
        working-directory: ./gemini-cli
        run: |
          npm ci
          npm run build:packages
          npm pack -w @google/gemini-cli --pack-destination ./packages/cli/dist
          npm pack -w @google/gemini-cli-core --pack-destination ./packages/core/dist

      - name: Get CLI version
        id: cli-version
        working-directory: ./gemini-cli
        run: |
          CLI_VERSION=$(node -p "require('./packages/cli/package.json').version")
          echo "CLI_VERSION=$CLI_VERSION" >> $GITHUB_OUTPUT
          echo "üì¶ Gemini CLI version from source: $CLI_VERSION"

      - name: Log in to Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Check if Gemini CLI image exists
        id: gemini-exists
        run: |
          echo "üîç Checking if we have built Gemini CLI version ${{ steps.cli-version.outputs.CLI_VERSION }}"
          echo "Looking for: ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ steps.cli-version.outputs.CLI_VERSION }}"
          if docker manifest inspect ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ steps.cli-version.outputs.CLI_VERSION }} > /dev/null 2>&1; then
            echo "‚úÖ We already have this version built, skipping build"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "üèóÔ∏è We haven't built this version yet, will build"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Set up Docker Buildx
        if: steps.gemini-exists.outputs.exists == 'false'
        uses: docker/setup-buildx-action@v3

      - name: Build and push Gemini CLI image
        if: steps.gemini-exists.outputs.exists == 'false'
        uses: docker/build-push-action@v5
        with:
          context: ./gemini-cli
          file: ./gemini-cli/Dockerfile
          platforms: linux/amd64,linux/arm64
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:latest
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ needs.version.outputs.version }}
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ steps.cli-version.outputs.CLI_VERSION }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          # Add registry cache for better layer reuse
          build-args: |
            BUILDKIT_INLINE_CACHE=1
            CLI_VERSION_ARG=${{ steps.cli-version.outputs.CLI_VERSION }}

      - name: Tag existing image with platform version
        if: steps.gemini-exists.outputs.exists == 'true' && github.event_name != 'pull_request'
        run: |
          echo "üè∑Ô∏è Tagging existing image with platform version"
          docker buildx imagetools create \
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ steps.cli-version.outputs.CLI_VERSION }} \
            --tag ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ needs.version.outputs.version }}

  # Deploy to single environment (only on main branch)
  deploy:
    needs: [version, build-claude-code, build-orchestrator, build-gemini-cli]
    if: (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v')) && github.event_name == 'push'
    runs-on: [self-hosted]
    steps:
      - uses: actions/checkout@v4

      - name: Setup tools
        run: |
          # Create local bin directory
          mkdir -p $HOME/bin

          # Copy tools from shared location if they exist
          if [ -f /shared/kubectl ]; then
            cp /shared/kubectl $HOME/bin/
            chmod +x $HOME/bin/kubectl
          fi

          if [ -f /shared/helm ]; then
            cp /shared/helm $HOME/bin/
            chmod +x $HOME/bin/helm
          fi

          # Add to PATH for this job
          echo "$HOME/bin" >> $GITHUB_PATH

      - name: Verify tools
        run: |
          echo "Checking tools..."
          kubectl version --client || echo "kubectl failed"
          helm version || echo "helm failed"

      - name: Verify Kubernetes access
        run: |
          echo "üîç Verifying Kubernetes access..."
          kubectl cluster-info

      - name: Install TaskRun CRD
        run: |
          echo "üìã Installing TaskRun CRD..."
          kubectl apply -f infra/crds/taskrun-crd.yaml

          # Wait for CRD to be established
          kubectl wait --for condition=established --timeout=60s crd/taskruns.orchestrator.io
          echo "‚úÖ TaskRun CRD installed successfully"

      - name: Deploy Orchestrator with Helm
        run: |
          echo "üöÄ Deploying platform version ${{ needs.version.outputs.version }}"

          # Deploy the orchestrator (TaskRun controller manages Claude Code and Gemini CLI agents)
          helm upgrade --install orchestrator ./infra/charts/orchestrator \
            --namespace orchestrator \
            --create-namespace \
            --set image.repository=${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/orchestrator \
            --set image.tag=${{ needs.version.outputs.version }} \
            --set secrets.anthropicApiKey="${{ secrets.ANTHROPIC_API_KEY }}" \
            --set secrets.githubToken="${{ secrets.GH_TOKEN_FOR_AGENTS }}" \
            --timeout 10m \
            --wait \
            --atomic

          # Apply the controller configuration
          echo "üìã Applying TaskRun controller configuration..."
          kubectl apply -f infra/crds/taskrun-controller-config.yaml

          echo "üìã Available agent images for TaskRun resources:"
          echo "- Claude Code: ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ needs.version.outputs.version }}"
          echo "- Gemini CLI: ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ needs.version.outputs.version }}"

      - name: Verify deployment
        run: |
          echo "üîç Verifying deployment..."
          kubectl get pods -n orchestrator
          kubectl get services -n orchestrator
          kubectl get taskruns -A || echo "No TaskRuns found (expected for fresh deployment)"

          # Wait for orchestrator to be ready
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=orchestrator -n orchestrator --timeout=300s

          echo "‚úÖ Deployment verification complete!"

      - name: Deployment notification
        run: |
          echo "üéâ Successfully deployed platform version ${{ needs.version.outputs.version }}"
          echo ""
          echo "Images deployed:"
          echo "- ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ needs.version.outputs.version }}"
          echo "- ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/orchestrator:${{ needs.version.outputs.version }}"
          echo "- ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ needs.version.outputs.version }}"
          echo "  (Gemini CLI also tagged with its official version from Google)"
          echo ""
          echo "Cluster status:"
          kubectl get pods -n orchestrator -o wide
          echo ""
          echo "üåê Orchestrator should be accessible through your Twingate network"

  # Create release (only for tags)
  release:
    needs: [version, deploy]
    if: needs.version.outputs.is-release == 'true'
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          tag_name: ${{ needs.version.outputs.version }}
          name: Release ${{ needs.version.outputs.version }}
          body: |
            ## Container Images

            This release includes the following container images:
            - `${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ needs.version.outputs.version }}`
            - `${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/orchestrator:${{ needs.version.outputs.version }}`
            - `${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ needs.version.outputs.version }}`

            ## Deployment

            Update your Helm values or Kubernetes manifests to use the new image tags.
          draft: false
          prerelease: false
